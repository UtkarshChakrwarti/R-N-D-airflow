# Airflow 2.6.3 on local Kubernetes with Argo CD (dual namespaces)

This PoC spins up two Airflow 2.6.3 environments on a single local Kubernetes cluster using the **KubernetesExecutor** and **Argo CD** as the only installer. Everything is GitOps-driven: no `helm install` or ad-hoc `kubectl apply` beyond the Argo CD bootstrap in `bootstrap/`.

Root layout (keep all three at repo root):
- `bootstrap/`: create the local cluster and install Argo CD once.
- `platform/`: Argo CD Applications plus Helm dependency stubs/values for the two Airflow releases.
- `dags/`: DAGs split by namespace (`cron` vs `user`) for Git sync.

## What you get
- Airflow **2.6.3** running the **KubernetesExecutor** in two namespaces: `airflow-cron` (scheduled DAGs) and `airflow-user` (manual/triggered DAGs).
- Git-driven DAG updates via `git-sync` sidecars pointing at `https://github.com/your-org/airflow-dags.git` (adjust as needed).
- In-memory PostgreSQL via the Airflow chart’s subchart (`persistence.enabled=false`) so the footprint stays small while remaining chart-compatible with production.

## Step-by-step PoC (copy/paste friendly)
1) **Create cluster + namespaces + Argo CD** (imperative once): follow `bootstrap/README.md` (Kind example provided). When done you should have namespaces `argocd`, `airflow-cron`, and `airflow-user`.

2) **Prepare the two repos Argo CD will watch**:
   - **Platform repo (this one)**: contains `bootstrap/`, `platform/`, `dags/`. Argo CD reads `platform/argo-apps` and installs the two Airflow releases.
   - **External DAGs repo**: point `dags.gitSync.repo` in `platform/airflow/*/values.yaml` to a **separate Git repo** that exposes `dags/cron` and `dags/user` subfolders. For a quick start, push the contents of this repo’s `dags/` folder to a new repo such as `https://github.com/your-org/airflow-dags.git`.
     - Private repo? Create one secret per namespace with either an SSH key (`ssh-privatekey` data key) referenced by `gitSync.sshKeySecret`, or basic auth (`GIT_SYNC_USERNAME` / `GIT_SYNC_PASSWORD`) referenced by `gitSync.credentialsSecret` in each values file.

3) **Point Argo CD at the platform folder (GitOps takes over)**:
   ```bash
   argocd repo add <YOUR_PLATFORM_REPO_URL>
   argocd app create airflow-platform \
     --repo <YOUR_PLATFORM_REPO_URL> \
     --path platform/argo-apps \
     --dest-server https://kubernetes.default.svc \
     --dest-namespace argocd \
     --sync-policy automated
   argocd app wait airflow-platform --health --timeout 300
   ```
   Argo CD will render the two `Application` objects from `platform/argo-apps` and install Airflow with the Helm dependency stubs under `platform/airflow/*`.

4) **Wait for Pods to become Ready** (per namespace) and count containers for your demo:
   ```bash
   kubectl get pods -n airflow-cron
   kubectl get pods -n airflow-user
   ```
   Expected steady-state per namespace (after the one-time `run-airflow-migrations` Job completes):
   - `airflow-scheduler-...` (1 pod, **2 containers**: scheduler + git-sync)
   - `airflow-webserver-...` (1 pod, **2 containers**: webserver + git-sync)
   - `airflow-triggerer-...` (1 pod, **2 containers**: triggerer + git-sync)
   - `airflow-postgresql-0` (1 pod, **1 container**)
   Total: **4 running pods / 7 containers per namespace**. Any new task will spin up **ephemeral KubernetesExecutor Pods** prefixed with `airflow-worker-...` in that namespace.

5) **Verify Airflow version + executor**:
   ```bash
   kubectl exec -n airflow-cron deploy/airflow-webserver -- airflow version
   kubectl exec -n airflow-cron deploy/airflow-webserver -- airflow config get-value core executor
   ```
   Both commands should report `2.6.3` and `KubernetesExecutor`.

6) **Confirm DAG sync + namespace routing**:
   - Cron DAGs: `dags/cron/example_cron_dag.py` -> `airflow-cron` namespace (scheduled every 5 minutes).
   - User DAGs: `dags/user/example_user_dag.py` -> `airflow-user` namespace (manual trigger only).
   - In the UI, expect `example_cron_dag` only in `airflow-cron` and `example_user_dag` only in `airflow-user`.

7) **Trigger a DAG and watch Pods/logs for the presentation**:
   ```bash
   # Cron namespace (manual trigger of the scheduled DAG)
   kubectl exec -n airflow-cron deploy/airflow-webserver -- \
     airflow dags trigger example_cron_dag
   kubectl get pods -n airflow-cron -w

   # User namespace (ad-hoc DAG)
   kubectl exec -n airflow-user deploy/airflow-webserver -- \
     airflow dags trigger example_user_dag
   kubectl get pods -n airflow-user -w
   ```
   You should see short-lived KubernetesExecutor Pods (one per task) named `airflow-worker-...` spin up in the same namespace as the DAG you triggered, then terminate after success. The pod’s single container writes a concise log showing the operator output.

8) **Inspect logs and task output** (per component). Each long-running pod emits continuous logs; task pods end with operator output. Useful commands to show during the demo:
   ```bash
   # Webserver, Scheduler, Triggerer (show last 50 lines)
   kubectl logs deploy/airflow-webserver -n airflow-cron --tail 50
   kubectl logs deploy/airflow-scheduler -n airflow-cron --tail 50
   kubectl logs deploy/airflow-triggerer -n airflow-cron --tail 50

   # A finished KubernetesExecutor task pod
   kubectl logs <airflow-worker-pod-name> -n airflow-cron

   # Same checks for the user namespace
   kubectl logs deploy/airflow-webserver -n airflow-user --tail 50
   kubectl logs deploy/airflow-scheduler -n airflow-user --tail 50
   kubectl logs deploy/airflow-triggerer -n airflow-user --tail 50
   kubectl logs <airflow-worker-pod-name> -n airflow-user
   ```

9) **Monitor health after deployment**:
   ```bash
   # Quick pod/status view
   kubectl get pods -n airflow-cron -o wide
   kubectl get pods -n airflow-user -o wide

   # Argo CD sync + health
   argocd app list

   # Check resource usage if you have metrics-server
   kubectl top pod -n airflow-cron
   kubectl top pod -n airflow-user
   ```

10) **Know where pods can schedule (taints / node pools)**:
    - Kind control-plane nodes are tainted by default (`node-role.kubernetes.io/control-plane:NoSchedule`). The values files already add matching tolerations so the PoC runs even on a single node.
    - On multi-node clusters, set a `nodeSelector` or `affinity` in each values file to steer cron vs. user workloads (e.g., `nodepool=cron` vs. `nodepool=user`).
    - If you add new taints, mirror them under the `tolerations` blocks so Argo CD keeps pods schedulable. To confirm placement during the demo:
      ```bash
      kubectl describe pod -n airflow-cron <pod-name> | grep -E "^Node:|Tolerations|Node-Selectors" -A3
      kubectl describe pod -n airflow-user <pod-name> | grep -E "^Node:|Tolerations|Node-Selectors" -A3
      ```

## Files you can reuse
- `bootstrap/README.md`: Kind cluster config + Argo CD bootstrap commands.
- `platform/argo-apps/*.yaml`: Argo CD Applications that deploy Airflow into `airflow-cron` and `airflow-user`.
- `platform/airflow/cron|user/Chart.yaml`: Helm dependency stubs to pull the official Airflow chart 1.13.1.
- `platform/airflow/cron|user/values.yaml`: Airflow 2.6.3 + KubernetesExecutor configuration, GitSync, in-memory Postgres.
- `dags/cron/example_cron_dag.py`: Scheduled DAG proving the cron namespace pipeline.
- `dags/user/example_user_dag.py`: Manual DAG for the user namespace.

## Swapping to production-like settings later
- Replace the embedded PostgreSQL block in `platform/airflow/*/values.yaml` with your managed DB connection secret and enable persistence.
- Tune `resources` per namespace; increase `parallelism`/`dag_concurrency` independently.
- Add OIDC/SAML auth by extending `config` and `webserver.extraEnv`.
- Wire external secrets (e.g., ESO) for credentials instead of inline values.
